{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np\n",
    "import functions as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from tqdm import tqdm\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in tqdm(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "data = load_vectors('cc.ko.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-356743ee24d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exception calling callback for <Future at 0x2429f6c9438 state=finished returned list>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\YOON\\anaconda3\\envs\\nlp\\lib\\site-packages\\joblib\\parallel.py\", line 808, in dispatch_one_batch\n",
      "    tasks = self._ready_batches.get(block=False)\n",
      "  File \"C:\\Users\\YOON\\anaconda3\\envs\\nlp\\lib\\queue.py\", line 161, in get\n",
      "    raise Empty\n",
      "queue.Empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\YOON\\anaconda3\\envs\\nlp\\lib\\site-packages\\joblib\\externals\\loky\\_base.py\", line 625, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"C:\\Users\\YOON\\anaconda3\\envs\\nlp\\lib\\site-packages\\joblib\\parallel.py\", line 347, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"C:\\Users\\YOON\\anaconda3\\envs\\nlp\\lib\\site-packages\\joblib\\parallel.py\", line 780, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"C:\\Users\\YOON\\anaconda3\\envs\\nlp\\lib\\site-packages\\joblib\\parallel.py\", line 819, in dispatch_one_batch\n",
      "    islice = list(itertools.islice(iterator, big_batch_size))\n",
      "  File \"<ipython-input-7-85a388cde409>\", line 16, in <genexpr>\n",
      "    embeddings_index = dict(Parallel(n_jobs=-1)(delayed(loading)(line) for line in f))\n",
      "  File \"C:\\Users\\YOON\\anaconda3\\envs\\nlp\\lib\\codecs.py\", line 321, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 3486: invalid continuation byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1173479\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    embeddings_index = {}\n",
    "\n",
    "    f = open(os.path.join('cc.ko.300.vec'), 'r', encoding='utf-8')\n",
    "    def loading(line):\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        return word, coefs\n",
    "\n",
    "    embeddings_index = dict(Parallel(n_jobs=-1)(delayed(loading)(line) for line in f))\n",
    "    f.close()\n",
    "    print(len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'짜증나다'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-37cf164f48d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membeddings_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'짜증나다'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: '짜증나다'"
     ]
    }
   ],
   "source": [
    "embeddings_index['짜증나다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "df_train = pd.read_pickle('token_train_data.pkl')\n",
    "df_test = pd.read_pickle('token_test_data.pkl')\n",
    "\n",
    "token_train_data, train_lable = df_train['tokens'].values.tolist(), df_train['labels']\n",
    "token_test_data, test_lable = df_test['tokens'].values.tolist(), df_test['labels']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "45505\n"
     ]
    }
   ],
   "source": [
    "simple_ko_vec = {}\n",
    "oov_list=[]\n",
    "simple_ko_vec['<PAD/>'] = ko_model.wv.word_vec('<PAD/>')\n",
    "for sent in token_train_data+token_test_data:\n",
    "    for token in sent:\n",
    "        try:\n",
    "            simple_ko_vec[token] = ko_model.wv.word_vec(token)\n",
    "        except:\n",
    "            oov_list.append(token)\n",
    "oov_list= list(set(oov_list))\n",
    "print(len(oov_list))\n",
    "print(len(simple_ko_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요단어 선별하여 pickle에 딕셔너리 저장 (총 단어수 : 31604)\n",
    "import pickle\n",
    "with open('simple_ko_vec.pkl','wb') as fw:\n",
    "    pickle.dump(simple_ko_vec, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('simple_ko_vec.pkl','rb') as fw:\n",
    "    simple_w2v= pickle.load(fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('token_train_data.pkl')\n",
    "df_test = pd.read_pickle('token_test_data.pkl')\n",
    "token_train_data, train_lable = df_train['tokens'], df_train['labels']\n",
    "token_test_data, test_lable = df_test['tokens'], df_test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sentences = fc.pad_sequence(token_train_data)\n",
    "paddedarray=[]\n",
    "for x in padded_sentences:\n",
    "    for token in x:\n",
    "        paddedarray.append(simple_w2v[token])\n",
    "# final_array = paddedarray.reshape(-1,max_len,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145791"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러들이는데 시간 오지게 걸리고? 별로 안걸리는데?, 다운 받아서 쥬피터 폴더에 넣어야함\n",
    "ko_model = models.fasttext.load_facebook_model('cc.ko.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt # 이걸 써서 포스 태깅을 할꺼고\n",
    "okt = Okt()\n",
    "import pandas as pd # 이것들 가져와\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장들 전처리하기\n",
    "def preprocessing(data):\n",
    "    data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    data = data.dropna(how = 'any')\n",
    "    data['document'] = data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "    data['document'].replace('', np.nan, inplace=True)\n",
    "    data = data.dropna(how = 'any')\n",
    "    sentences = data['document'].tolist()\n",
    "    label = data['label']\n",
    "    print('data len = {}'.format(len(sentences)))\n",
    "    return sentences, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 토큰화 하기\n",
    "def tokenize(sentence):\n",
    "    okt = Okt()\n",
    "    tokenized_sentence = []\n",
    "\n",
    "    # 우선 단어의 기본형으로 모두 살리고, 명사, 동사, 영어만 담는다.\n",
    "    # 그냥 nouns로 분리하는 것보다 좀 더 정확하고 많은 데이터를 얻을 수 있다.\n",
    "    for line in sentence:\n",
    "        result = []\n",
    "        temp_sentence = okt.pos(line, norm=True, stem=True) # 먼저 형태소 분리해서 리스트에 담고\n",
    "\n",
    "        for i in temp_sentence:                             \n",
    "            if (i[1] == 'Noun' or i[1] == 'Adjective' or i[1] == 'Alpha'):                  \n",
    "                result.append(i[0])\n",
    "            \n",
    "        tokenized_sentence.append(result)\n",
    "\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sentences, padding_word=\"<PAD/>\"): #  오른쪽을 패딩주기\n",
    "    maxlen = 40\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        if len(sentence)<=maxlen:\n",
    "            num_padding = maxlen - len(sentence)\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "        else : new_sentence = sentence[:maxlen]\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ko_model.wv.word_vec('<PAD/>')\n",
    "paddedarray = np.array([ ko_model.wv.word_vec(token) for x in padded for token in x])\n",
    "final_array=paddedarray.reshape(-1,max_len,300) # 이런 형태네? max_length of sent = 22(34),근데 10개의 sent, 22*10 fasttext = 300(vector) \n",
    "final_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(padded_sentences):\n",
    "    paddedarray = np.array([ ko_model.wv.word_vec(token) for x in padded_sentences for token in x])\n",
    "    final_array=paddedarray.reshape(-1,max_len,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pre_train_data, train_label = preprocessing(train_data)\n",
    "pre_test_data, test_label = preprocessing(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_train_data = tokenize(pre_train_data)\n",
    "token_test_data = tokenize(pre_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len']=[len(i) for i in df['tokens']]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['len'].hist(bins =30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 토큰화된 데이터 pickle로 저장\n",
    "# import pandas as pd\n",
    "# import pickle\n",
    "# df = pd.DataFrame(np.array(token_test_data),columns=['tokens'])\n",
    "# df['labels']=list(map(int,test_label))\n",
    "# df2 = pd.DataFrame(token_test_data,test_label,columns=['tokens','lable'])\n",
    "# df1.to_pickle('token_train_data.pkl')\n",
    "# df2.to_pickle('token_test_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "df_train = pd.read_pickle('token_train_data.pkl')\n",
    "df_test = pd.read_pickle('token_test_data.pkl')\n",
    "\n",
    "token_train_data, train_lable = df_train['tokens'], df_train['labels']\n",
    "token_test_data, test_lable = df_test['tokens'], df_test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as fc\n",
    "max_len=30\n",
    "final_train_data = fc.fasttext_vectorize(fc.pad_sequence(token_train_data), max_len=max_len)\n",
    "final_test_data = fc.fasttext_vectorize(fc.pad_sequence(token_test_data), max_len=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "embedding_dim = 200\n",
    "filter_sizes = (3, 4, 5)\n",
    "num_filters = 100\n",
    "dropout = 0.5\n",
    "hidden_dims = 10\n",
    "\n",
    "batch_size = 50\n",
    "num_epochs = 10\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "conv_blocks = []\n",
    "\n",
    "sequence_length = 200\n",
    "\n",
    "# input_shape = (sequence_length, embedding_dim) # input shape for\n",
    "input_shape = (40, 300) # input shape for data, (max_length of sent, vect)\n",
    "\n",
    "model_input = keras.layers.Input(shape=input_shape)\n",
    "\n",
    "z = model_input\n",
    "for sz in filter_sizes:\n",
    "    conv = keras.layers.Conv1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = keras.layers.MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = keras.layers.Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "z = keras.layers.Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "z = keras.layers.Dense(hidden_dims, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.003), bias_regularizer=keras.regularizers.l2(0.003))(z)\n",
    "z = keras.layers.Dropout(dropout)(z)\n",
    "model_output = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = keras.Model(model_input, model_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir = './ckpt1'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=0),\n",
    "    # This callback saves a SavedModel every 100 batches.\n",
    "    # We include the training loss in the folder name.\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_dir + '/ckpt-loss={loss:.3f}',\n",
    "        save_freq=500)\n",
    "]\n",
    "history = model.fit(final_train_data, train_lable, epochs=10, callbacks=callbacks, batch_size = batch_size, validation_data=(final_test_data, test_lable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_' + string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_' + string])\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.epoch = list(map(lambda x: x+25, history.epoch))\n",
    "history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'val_accuracy':[],'accuracy':[],'epoch':[],'loss':[],'val_loss':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.keys():\n",
    "    if i == 'epoch':\n",
    "        data['epoch'].extend(history.epoch)\n",
    "    else:\n",
    "        data[i].extend(history.history[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.DataFrame(data)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new=pd.concat([new,b])\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.set_index('epoch', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_graphs(data, string):\n",
    "  plt.plot(data[string])\n",
    "  plt.plot(data['val_' + string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_' + string])\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(new, 'accuracy')\n",
    "plot_graphs(new, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import tokenize, m2_load_token_and_label\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "\n",
    "oov_tok = '<OOV>'\n",
    "truct_type = 'post'\n",
    "padding_type = 'post'\n",
    "max_length = 30\n",
    "vocab_size =20000\n",
    "training_sentences, training_labels, testing_sentences, testing_labels = m2_load_token_and_label()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_idx = tokenizer.index_word\n",
    "\n",
    "with open('word_idx.pkl','wb') as fw:\n",
    "    pickle.dump(word_idx, fw)\n",
    "\n",
    "# test_sent=['재밌어요','재미 없어요']\n",
    "\n",
    "# test_sent = tokenize(test_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40014"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '<OOV>', 2: '재밌다', 3: '재미', 4: '없다'}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_sequences  = tokenizer.texts_to_sequences(test_sent)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, \n",
    "                                  padding=padding_type, truncating=truct_type)\n",
    "\n",
    "vocab_size = len(word_idx) + 1\n",
    "embedding_dim = 300\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "with open('simple_ko_vec.pkl','rb') as fw:\n",
    "    ko_model= pickle.load(fw)\n",
    "\n",
    "\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    embedding_vector = ko_model[word] if word in ko_model else None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "#################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.10251483, -0.1379561 ,  0.06352632, ...,  0.14521724,\n",
       "        -0.06100887,  0.08875071],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-98eae484bc78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msz\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilter_sizes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     embedding = keras.layers.Embedding(embedding_matrix.shape[0],embedding_matrix[1],input_length=max_length,\n\u001b[1;32m---> 16\u001b[1;33m                                       weights =embedding_matrix, trainable = False)(z)\n\u001b[0m\u001b[0;32m     17\u001b[0m     conv = keras.layers.Conv1D(filters=num_filters,\n\u001b[0;32m     18\u001b[0m                       \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\embeddings.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, activity_regularizer, embeddings_constraint, mask_zero, input_length, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_shape'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m       raise ValueError('Both `input_dim` and `output_dim` should be positive, '\n\u001b[0;32m    106\u001b[0m                        'found input_dim {} and output_dim {}'.format(\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "embedding_dim = 200\n",
    "filter_sizes = (3, 4, 5)\n",
    "num_filters = 100\n",
    "dropout = 0.5\n",
    "hidden_dims = 100\n",
    "max_length = 30\n",
    "\n",
    "conv_blocks =[]\n",
    "input_shape = (max_length)\n",
    "model_input = keras.layers.Input(shape=input_shape)\n",
    "z = model_input\n",
    "\n",
    "\n",
    "for sz in filter_sizes:\n",
    "    embedding = keras.layers.Embedding(embedding_matrix.shape[0],embedding_matrix[1],input_length=max_length,\n",
    "                                      weights =[embedding_matrix], trainable = False)(z)\n",
    "    conv = keras.layers.Conv1D(filters=num_filters,\n",
    "                      kernel_size=sz,\n",
    "                      padding=\"valid\",\n",
    "                      activation=\"relu\",\n",
    "                      strides=1)(embedding)\n",
    "    conv = keras.layers.GlobalAveragePooling1D()(conv)\n",
    "    conv = keras.layers.Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "z = keras.layers.Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = keras.layers.Dense(hidden_dims, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.003), bias_regularizer=tf.keras.regularizers.l2(0.003))(z)\n",
    "z = keras.layers.Dropout(dropout)(z)\n",
    "model_output = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "model = keras.Model(model_input, model_output)\n",
    "\n",
    "batch_size = 50\n",
    "num_epochs = 10\n",
    "min_word_count = 1\n",
    "context = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['재밌다'], ['재미', '없다']]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokend"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
